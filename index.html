
<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
    <meta property="og:image" content="./images/uw-crest-web.png" />
    <link rel="icon" type="image/png" href="./images/favicon.ico" />
    <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
    <meta property="og:title" content="CS766 - Final Project" />
    <meta property="og:description" content="This is the website for Final Project of CS766: Computer Vision (Spring 2023) @ UW-Madison" />
    <title>cs766</title>
    <!-- For debug only -->
    <!-- <meta http-equiv=”refresh” content="5" /> -->

    <!-- <link href="base.css" type="text/css" rel="StyleSheet"> -->
    <link href="index.css" type="text/css" rel="StyleSheet">
    <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
    <script type="text/javascript" src="jquery.mlens-1.0.min.js"></script>
    <script type="text/javascript" src="jquery.js"></script>
    <!-- Global site tag (gtag.js) - Google Analytics -->
		<!--
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-98008272-2"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'UA-98008272-2');
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
    </script>
		-->

</head>

<body>

    <div class="content">
			<h1>Exploring the Potential of Pretrained Stable Diffusion <br> for Manipulating Facial Attributes</h1>
            <p><center>Final Project for <a href='https://pages.cs.wisc.edu/~mohitg/courses/CS766/'> CS766: Computer Vision</a> (Spring 2023)</center></p>
        <p id="authors">
             
             <!-- <br> -->
            <font size="+1">
            <a href="https://thaoshibe.github.io/">Thao Nguyen</a>
            <a href="https://geography.wisc.edu/staff/ji-yuhan/">Yuhan Ji</a>
            <a href="https://www.linkedin.com/in/qi-yao-4a036917a/">Qi Yao</a>
<!-- 						<a href="http://people.csail.mit.edu/jwulff/">Jonas Wulff</a>
            <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> -->
            <!-- <br> -->
            </font>
            <br>
            University of Wisconsin - Madison
			<br>
            <img src='./images/uw-logo-flush-web.png' height="80px"></img>
                        <!-- <i>International Conference on Learning Representations 2021</i> -->
        </p>
				<font size="+2">
					<p style="text-align: center;">
						<a href="" target="_blank">[Video]</a> &nbsp;&nbsp;&nbsp;&nbsp;
						<a href="https://docs.google.com/presentation/d/1fIEMHppZbh9MpYMAh1s8RvmF3nrTVeaJ61ZR9ZAhI0A/edit?usp=sharing">[Slides]</a> &nbsp;&nbsp;&nbsp;&nbsp;
						<a href="" target="_blank">[Proposal]</a> &nbsp;&nbsp;&nbsp;&nbsp;
						<a href="" target="_blank">[Mid-term Report]</a>
                        &nbsp;&nbsp;&nbsp;&nbsp;
						<a href="logs.html" target="_blank">[Logs]</a>
						<!--
						<a href="TODO: youtube link?" target="_blank">[Video]</a>
						-->
					</p>
				</font>
				<font size="+1">
					<p style="text-align: center;">
						Skip to:  &nbsp;&nbsp;
						<a href="#abstract">[Abstract]</a> &nbsp;&nbsp;&nbsp;&nbsp;
						<a href="#framework">[Framework]</a> &nbsp;&nbsp;&nbsp;&nbsp;
						<a href="#evaluation">[Evaluation]</a> &nbsp;&nbsp;&nbsp;&nbsp;
                        <a href="#qualitative-results">[Qualitative Results]</a> &nbsp;&nbsp;&nbsp;&nbsp;
                        <a href="#quantitative-results">[Qualitative Results]</a> &nbsp;&nbsp;&nbsp;&nbsp;
                        <a href="#related-work">[Related Works]</a> &nbsp;&nbsp;&nbsp;&nbsp;
					</p>
				</font>
        <p>
            <!-- <img class='teaser-img' src='./images/uw-logo-flush-web.png' height="200px"></img> -->
        </p>

				<p id="abstract"><strong>Abstract: </strong>
				Facial image manipulation is the task of modifying a person's facial attributes like expressions, accessories, etc. while maintaining the identity and other irrelevant features.
                A text prompt will be provided as the guidance on how the image should be modified.
				</p>
                <center>
                <img src='./images/example_2.png' width="50%"></img>
                </center>
                <!-- <embed src='./images/example_2.pdf' width="100%" /> -->
                <!-- <iframe src='./images/example_2.pdf' style="width: 100%;height: 100%;border: none;"></iframe> -->
        <br clear="all">
    </div>
    <div class="content">
    <h2 id='framework'>Proposed Framework</h2>
    <p> Given an input image $I$ and editing text instruction $t$, our aim is to produce a new image $\hat{I}$ (shown in Figure \ref{fig:example}). While $\hat{I}$ should be edited according to the text $t$, other factors of image $I$ should be the same.
    </p>

    <p>Figure below shows the framework of the model, which contains two major parts: First, fine-tune the pre-trained stable diffusion model, that is, given one image, we need to implant the new ``word" that represents the face into the diffusion model's dictionary. Second, infer the output image conditioned on the face identifier, image description, and textual prompt.</p>
    <center>
    <!-- <center> -->
    <img src='./images/framework.png' width="80%"></img>
    </center>
    </div>

    </div>
    <div class="content">
    <h2 id='evaluation'>Evaluation</h2>
    <p>Our process involves computing embeddings for an image and text using a pre-trained CLIP model \cite{clip}. Since CLIP embeddings have the property of being identical for both text and image, we can use cosine similarity on that embedding space to determine how aligned the text is with the image. For instance, if we have a photo of a woman and the text description ``smile", the cosine similarity would indicate the likelihood of the woman smiling. The cosine similarity score ranges from -1 to 1, with 1 indicating a high likelihood and -1 suggesting no relation.</p>
    <center>
    <!-- <center> -->
    <img src='./images/evaluationpipeline.png' width="50%"></img>
    </center>
    </div>
    <div class="content">
    <h2 id='qualitative-results'>Qualitative Results</h2>
    <p>Our process involves computing embeddings for an image and text using a pre-trained CLIP model \cite{clip}. Since CLIP embeddings have the property of being identical for both text and image, we can use cosine similarity on that embedding space to determine how aligned the text is with the image. For instance, if we have a photo of a woman and the text description ``smile", the cosine similarity would indicate the likelihood of the woman smiling. The cosine similarity score ranges from -1 to 1, with 1 indicating a high likelihood and -1 suggesting no relation.</p>
    <center>
    <!-- <center> -->
    <img src='./images/uw-logo-flush-web.png' width="50%"></img>
    </center>
    </div>

    <div class="content">
    <h2 id='quantitative-results'>Quantitative Results</h2>
    <p>Our process involves computing embeddings for an image and text using a pre-trained CLIP model \cite{clip}. Since CLIP embeddings have the property of being identical for both text and image, we can use cosine similarity on that embedding space to determine how aligned the text is with the image. For instance, if we have a photo of a woman and the text description ``smile", the cosine similarity would indicate the likelihood of the woman smiling. The cosine similarity score ranges from -1 to 1, with 1 indicating a high likelihood and -1 suggesting no relation.</p>
    <center>
    <!-- <center> -->
    <img src='./images/uw-logo-flush-web.png' width="50%"></img>
    </center>
    </div>

    <div class="content">
    <h2 id='related-work'>Related Works</h2>
    <p><strong>Facial Editing</strong> Over the past few years, Generative Adversarial Models (GANs), particularly StyleGAN \cite{stylegan}, have been widely studied and used for facial image editing. However, the use of GAN models is limited to the learned latent space, which poses challenges in identifying manipulation directions within that space\cite{feat,styleencoder}. In addition, GANs require an image inversion process that may accumulate errors in the editing process \cite{pti,e4e}.
    While GANs naturally do not support textual input, most existing works lie on exploiting CLIP \cite{clip} to enable GANs to take text prompts as guidance \cite{styleclip}.
    On the other hand, as trained on billions text-image pairs, diffusion models inherently understand the text-image relationship. Diffusion models \cite{2021diff} have become a better alternative to GANs due to their image quality and interpretability. Text-to-image diffusion models such as DALL-E 2 \cite{ramesh2022hierarchical}, Imagen \cite{saharia2022photorealistic}, Mid-Journey\footnote{\url{https://www.midjourney.com/}}, and Stable Diffusion \cite{stablediffusion}, have proved the power of image generation under the guidance of textual prompts.</p>

    <p><strong>Diffusion Models</strong> Among diffusion models, Stable Diffusion \cite{stablediffusion} is the most popular and can generate high-quality images, but its use in editing images remains an active question.
    Diff-AE \cite{diffae} proposed to train an autoencoder module to capture the input characters. However, Diff-AE's image quality is not pleasant and manipulation is limited as it based on StyleGAN directions. The state-of-the-art model, InstructPix2Pix \cite{instructpix2pix}, introduced a new synthetic dataset of before-after images to fine-tune Stable Diffusion Model.
    It has been proven that fine-tuning a model may have a negative impact on its robustness.
    InstructPix2Pix \cite{instructpix2pix} performs well under common basic settings (for example, ``add glasses"), but struggles with out-of-domain editing, such as ``blue hair" (as depicted in Figure \ref{fig:baseline}).
    Although Original Stable Diffusion can comprehend the ``blue hair" attribute, InstructPix2Pix cannot replicate this extreme editing, as it may not appear frequently enough in the training sets.</p>
    <center>

    <p><strong>Textual Inversion</strong> DreamBooth \cite{ruiz2022dreambooth} a module that perform the textual inversion given a collection of the images as reference to a concept. The module can bind a unique identifier with a given input by fine-tuning the stable diffusion model, personalizing the general model in a few shot scenario. A new concept is introduce to the model by leveraging the prior shared knowledge from the class descriptor and then extracting the personalized knowledge by contrasting the images from other images from the same class. As a result, the identifier can well reconstruct the face given representative input.
    </p>
    <p><strong>Image Captioning</strong> Natural Language Generation (NLG) techniques will be applied to generate the linguistic contents of the input face image. Given the dataset of face-description pairs, we are able to fine-tune a pre-trained image captioning model specifically for the face class \cite{kumar2022imagecaptioning}. The pre-trained Vision Encoder Decoder Models, in which the encoder is used to encode the image, after which an autoregressive language model generates the caption.</p>
    <!-- <center> -->
    <img src='./images/ddim.png' width="60%"></img>
    </center>
    </div>
<!--     <div class="content" id="summary">

        <h2 style="text-align:center;">Summary</h2>

				<div class="container">
					<div class="image">
						<img src='img/teaser.gif'></img>
					</div>
					<div class="text">
						<p> We use a latent regressor network that learns from missing data for image composition and image completion.  The combination of the regressor network and a pretrained GAN forms an image prior to create realistic images despite unrealistic input. This animation briefly demonstrates some applications of our method.</p>
					</div>
				</div>
        <br>
        <hr>
        <p style="text-align: center;">Using the latent regression and pretrained GAN, we can create automatic collages and merge them into coherent composite images.</p>
        <img class='summary-img' src='img/website_composition.jpeg'></img>

        <br>
        <hr>
        <p style="text-align: center;">We demonstrate an application of editing and merging real images. <br>To better fit a specific image, we do a few seconds of initial optimization, and the remaining editing occurs in real-time.</p>
        <img class='summary-img' style="width:80%;" src='img/finetune_edit.jpeg'></img>
        <br>
        <hr>
        <p style="text-align: center;">We use the latent regressor to investigate the independently changeable parts that the GAN learns from data. For example, we visualize what regions of a given image change, when the outlined red portion is modified. The resulting variations show regions of the images that commonly vary together, which can be interpreted as a form of unsupervised part discovery.</p>
        <img class='summary-img' src='img/website_independence.jpeg'></img>
			</div>
			<div class="content" id="samples">
        <h2 style="text-align:center;">Additional Samples</h2>
				<p style="text-align: center;">Click on the panels below to view uncurated composite images generated from randomly sampled image parts.</p>
				<table style="text-align: center;" class="center">
					<tr>
						<td>
							<div class="boxshadow">
							<a href="http://latent-composition.csail.mit.edu/web_samples/samples/proggan_church/">
								<p style="text-align: center;">ProGAN Church</p>
								<img src="img/samples_proggan_church_sample000002_inverted_RGBM.png" style="width:150px">
							</a>
							</div>
						</td>
						<td>
							<div class="boxshadow">
							<a href="http://latent-composition.csail.mit.edu/web_samples/samples/proggan_livingroom/">
								<p style="text-align: center;">ProGAN Living Room</p>
								<img src="img/samples_proggan_livingroom_sample000000_inverted_RGBM.png" style="width:150px">
							</a>
							</div>
						</td>
						<td>
							<div class="boxshadow">
							<a href="http://latent-composition.csail.mit.edu/web_samples/samples/proggan_celebahq/">
								<p style="text-align: center;">ProGAN CelebAHQ</p>
								<img src="img/samples_proggan_celebahq_sample000008_inverted_RGBM.png" style="width:150px">
							</a>
							</div>
						</td>
					</tr>
					<tr class="spacertr"></tr>
					<tr>
						<td>
							<div class="boxshadow">
							<a href="http://latent-composition.csail.mit.edu/web_samples/samples/stylegan_church/">
								<p style="text-align: center;">StyleGAN Church</p>
								<img src="img/samples_sgan_church_sample000000_inverted_RGBM.png" style="width:150px">
							</a>
							</div>
						</td>
						<td>
							<div class="boxshadow">
							<a href="http://latent-composition.csail.mit.edu/web_samples/samples/stylegan_car/">
								<p style="text-align: center;">StyleGAN Car</p>
								<img src="img/samples_sgan_car_sample000006_inverted_RGBM.png" style="width:150px">
							</a>
							</div>
						</td>
						<td>
							<div class="boxshadow">
							<a href="http://latent-composition.csail.mit.edu/web_samples/samples/stylegan_ffhq/">
								<p style="text-align: center;">StyleGAN FFHQ</p>
								<img src="img/samples_sgan_ffhq_sample000011_inverted_RGBM.png" style="width:150px">
							</a>
							</div>
						</td>
					</tr>
				</table>
				<br>
        <hr>
				<p style="text-align: center;">Click on the panels below to view comparisons of different image reconstruction methods operating on the same randomly sampled real image parts. Composition is a balance between unifying the input parts to create a realistic output, but still remaining close to the input parts; the methods exhibit a tradeoff of these two factors. A third axis is inference time. In the below webpages, methods that require additional per-image optimization are marked with (*), whereas the other methods operate using a single forward pass. </p>
				<table style="text-align: center;" class="center">
					<tr>
						<td>
							<div class="boxshadow">
							<a href="http://latent-composition.csail.mit.edu/web_samples/comparisons/celebahq_face/">
								<p style="text-align: center;">CelebAHQ Faces</p>
								<img src="img/samples_celebahq_face_input.png" style="width:150px">
							</a>
							</div>
						</td>
						<td class="spacertd">
						<td>
							<div class="boxshadow">
							<a href="http://latent-composition.csail.mit.edu/web_samples/comparisons/lsun_church/">
								<p style="text-align: center;">LSUN Church</p>
								<img src="img/samples_lsun_church_input.png" style="width:150px">
							</a>
							</div>
						</td>
					</tr>
				</table>
    </div>      --> 
    <!-- <div class="content" id="references">

        <h2>Reference</h2>

				<p>L Chai, J Wulff, P Isola. Using latent space regression to analyze and leverage compositionality in GANs. <br>International Conference on Learning Representations, 2021.</p>

        <code>
			@inproceedings{chai2021latent,<br>
				&nbsp;&nbsp;title={Using latent space regression to analyze and leverage compositionality in GANs.},<br>
				&nbsp;&nbsp;author={Chai, Lucy and Wulff, Jonas and Isola, Phillip},<br>
				&nbsp;&nbsp;booktitle={International Conference on Learning Representations},<br>
				&nbsp;&nbsp;year={2021}<br>
			 }
				</code>
    </div>       -->
    <div class="content" id="acknowledgements">
          <p><strong>Acknowledgements</strong>:
					We would like to thank [] for helpful discussions and feedback. Thanks to [].
					This website template is adopted from <a href="https://chail.github.io/patch-forensics/">this template</a> and <a href='https://pages.cs.wisc.edu/~mohitg/courses/CS766/'>this template</a>.</p>
    </div>
</body>

</html>
