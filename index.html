<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta property="og:image" content="./images/uw-crest-web.png" />
    <!-- <meta > -->
    <link rel="icon" type="image/png" href="./images/favicon.ico" />
    <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
    <meta property="og:title" content="CS766 - Final Project" />
    <meta property="og:description" content="This is the website for Final Project of CS766: Computer Vision (Spring 2023) @ UW-Madison" />
    <title>CS 766</title>
    <!-- For debug only -->
    <!-- <meta http-equiv=â€refreshâ€ content="5" /> -->

    <!-- <link href="base.css" type="text/css" rel="StyleSheet"> -->
    <link href="index.css" type="text/css" rel="StyleSheet">
    <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
    <script type="text/javascript" src="jquery.mlens-1.0.min.js"></script>
    <script type="text/javascript" src="jquery.js"></script>
    <!-- Global site tag (gtag.js) - Google Analytics -->
		<!--
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-98008272-2"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'UA-98008272-2');
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
    </script>
		-->

</head>

<body>

    <div class="content">
			<h1>Exploring the Potential of Pretrained Stable Diffusion <br> for Manipulating Facial Attributes</h1>
            <p><center>Final Project for <a href='https://pages.cs.wisc.edu/~mohitg/courses/CS766/'> CS766: Computer Vision</a> (Spring 2023)</center></p>
        <p id="authors">
             
             <!-- <br> -->
            <font size="+1">
            <a href="https://thaoshibe.github.io/">Thao Nguyen</a>
            <a href="https://geography.wisc.edu/staff/ji-yuhan/">Yuhan Ji</a>
            <a href="https://www.linkedin.com/in/qi-yao-4a036917a/">Qi Yao</a>
<!-- 						<a href="http://people.csail.mit.edu/jwulff/">Jonas Wulff</a>
            <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> -->
            <!-- <br> -->
            </font>
            <br>
            University of Wisconsin - Madison
			<br>
            <img src='./images/uw-logo-flush-web.png' height="80px"></img>
                        <!-- <i>International Conference on Learning Representations 2021</i> -->
        </p>
				<font size="+2">
					<p style="text-align: center;">
						<a href="https://drive.google.com/file/d/1yt6w1viBGIjWvkFd9YZCimpi4_M7Q6D-/view?usp=sharing" target="_blank">[Video ğŸ¥]</a> &nbsp;&nbsp;&nbsp;&nbsp;
						<a href="https://drive.google.com/file/d/18FT2p5xlaxzUMoFRNjnH3eJ-mObOuoYS/view?usp=sharing" target="_blank">[Slides]</a> &nbsp;&nbsp;&nbsp;&nbsp;
						<a href="https://drive.google.com/file/d/1XkCAc-s3fmPjZ7NJr7rB1uIeWv57FWpY/view?usp=sharing" target="_blank">[Proposal]</a> &nbsp;&nbsp;&nbsp;&nbsp;
						<a href="https://drive.google.com/file/d/1wDn8UVKp7UW-3zqLczgaHQmNdnhiaJs7/view?usp=sharing" target="_blank">[Mid-term Report ğŸ“]</a>
                        &nbsp;&nbsp;&nbsp;&nbsp;
                        <a href="#implementation">[Code]</a>
                        &nbsp;&nbsp;&nbsp;&nbsp;
						<!-- <a href="logs.html" target="_blank">[More Results ğŸ–¼ï¸]</a> -->
						<!--
						<a href="TODO: youtube link?" target="_blank">[Video]</a>
						-->
					</p>
				</font>
				<font size="+1">
					<p style="text-align: center;">
						Skip to:  &nbsp;&nbsp;
						<!-- <a href="#abstract">[Abstract]</a> &nbsp;&nbsp;&nbsp; -->
						<a href="#framework">[Framework]</a> &nbsp;&nbsp;
						<a href="#evaluation">[Evaluation]</a> &nbsp;&nbsp;
                        <a href="#qualitative-results">[Qualitative ğŸ–¼ï¸]</a> &nbsp;&nbsp;
                        <a href="#quantitative-results">[Quantitative ğŸ“Š]</a> &nbsp;&nbsp;
                        <a href="#related-work">[Related Works ğŸ“–]</a> &nbsp;&nbsp;
                        <a href="#interesting-findings">[Interesting Findings ğŸ“]</a> &nbsp;&nbsp;
					</p>
				</font>
        <p>
            <!-- <img class='teaser-img' src='./images/uw-logo-flush-web.png' height="200px"></img> -->
        </p>

				<p id="abstract"><strong>Abstract: </strong>
				Facial image manipulation is the task of modifying a person's facial attributes like expressions, accessories, etc. while maintaining the identity and other irrelevant features.
                A text prompt will be provided as the guidance on how the image should be modified.
                In this project, we explore the use of Stable Diffusion model for facial editing.
				</p>
                <center>
                <img src='./images/example_2.png' width="50%"></img>
                </center>
                <!-- <embed src='./images/example_2.pdf' width="100%" /> -->
                <!-- <iframe src='./images/example_2.pdf' style="width: 100%;height: 100%;border: none;"></iframe> -->
        <br clear="all">
    </div>
    <div class="content">
    <h2 id='framework'>ğŸ§¾ Proposed Framework</h2>
    <p> Given an input image I and editing text instruction t, our aim is to produce a new image I'. While I' should be edited according to the text t, other factors of image I should be the same.
    </p>

    <p>Figure below shows the framework of the model, which contains two major parts: First, fine-tune the pre-trained stable diffusion model, that is, given one image, we need to implant the new "word" that represents the face into the diffusion model's dictionary. Second, infer the output image conditioned on the face identifier and textual prompt.</p>
    <center>
    <!-- <center> -->
    <img src='./images/framework.png' width="80%"></img>
    </center>
    </div>

    </div>
    <div class="content">
    <h2 id='evaluation'>ğŸ“ Evaluation</h2>
    <p>Our process involves computing embeddings for an image and text using a pre-trained <a href="https://openai.com/research/clip">CLIP model</a>. Since CLIP embeddings have the property of being identical for both text and image, we can use cosine similarity on that embedding space to determine how aligned the text is with the image. For instance, if we have a photo of a woman and the text description "A photo of a smiling woman", the cosine similarity will indicate the likelihood of the woman smiling. The cosine similarity score ranges from -1 to 1, with 1 indicating a high likelihood and -1 suggesting no relation.</p>
    <center>
    <!-- <center> -->
    <img src='./images/evaluationpipeline.png' width="50%"></img>
    </center>
    </div>
    <div class="content">
    <h2 id='qualitative-results'>ğŸ–¼ Qualitative Results</h2>
    <p>Qualitative results demonstrate that our method can generate images that are <b>consistent with the given text prompt</b>.
    <br>
    Especially in <b>Angry ğŸ˜ , Blue Glasses ğŸ‘“, Red Glasses ğŸ‘“, and Blone Hair ğŸŸ¨</b> cases.
    <br>
    Note: All test images have size 512x512, but in here we have manually downscaled them to 300x300 for quicker web rendering. For higher resolution images, please refer to this <a href='https://drive.google.com/drive/folders/1oqTnnLQ-ZzbHOvALEM7prZyCsg3lpCuN?usp=share_link'>ğŸš€ Google Drive for baseline</a>, or this <a href="https://drive.google.com/drive/folders/1GLeArPkx_K1_X4afLzOWerFg-9UHbqjc?usp=share_link"> ğŸš€ Google Drive for ours</a>.</p>
    </p>
		<table>
			<center>
		<!-- <thead> -->
		<tr>
			<!-- <th> </th> -->
			<th colspan="1">Input Image</td>
			<th colspan="1"><img src='./images/00007.png' width="120px"></th>
			<th></th>
			<th></th>
			<th></th>
			<th></th>
		</tr>
		  <tr>
		    <td><center>Prompt</center></td>
		    <td><center>Angry ğŸ˜ </center></td>
		    <td><center>Mustache ğŸ¥¸</center></td>
		    <td><center>Blue Glasses ğŸ‘“</center></td>
		    <td><center>Red Glasses ğŸ‘“</center></td>
		    <td><center>Red Lipstick ğŸ’‹</center></td>
		    <td><center>Blone Hair ğŸŸ¨</center></td>
		  </tr>
		<!-- </thead> -->
		<tbody>
		  <tr>
		    <td><center>Baseline</center></td>
		    <td><img src='./images/qualitative/baseline/00007_angry.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/baseline/00007_mustache.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/baseline/00007_blueeyeglasses.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/baseline/00007_redeyeglasses.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/baseline/00007_redlipstick.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/baseline/00007_blondehair.png' width="150px"></img></td>
		  </tr>
		  <tr>
		    <td><center><b>Ours â­</b></center></td>
		    <td><img src='./images/qualitative/ours/00007_angry.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/ours/00007_mustache.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/ours/00007_blueeyeglasses.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/ours/00007_redeyeglasses.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/ours/00007_redlipstick.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/ours/00007_blondehair.png' width="150px"></img></td>
		  </tr>
		</tbody>
		<tr>
			<!-- <td colspan="7"><hr></td> -->
		</tr>
<!-- 		<tr>
			<th colspan="1">Input Image</td>
			<th colspan="1"><img src='./images/00004.png' width="120px"></th>
			<th></th>
			<th></th>
			<th></th>
			<th></th>
		</tr> -->
<!-- 		  <tr>
		    <td><center>Prompt</center></td>
		    <td><center>Angry ğŸ˜ </center></td>
		    <td><center>Blue Hair ğŸ”·</center></td>
		    <td><center>Black Glasses </center></td>
		    <td><center>Red Glasses</center></td>
		    <td><center>Wearing Earrings</center></td>
		    <td><center>Wearing Hat ğŸ©</center></td>
		  </tr> -->
		<!-- </thead> -->
		<!-- <tbody> -->
<!-- 		  <tr>
		    <td><center>Baseline</center></td>
		    <td><img src='./images/qualitative/baseline/00004_angry.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/baseline/00004_mustache.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/baseline/00004_blueeyeglasses.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/baseline/00007_redeyeglasses.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/baseline/00007_wearingearrings.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/baseline/00007_wearinghats.png' width="150px"></img></td>
		  </tr> -->
<!-- 		  <tr>
		    <td><center>Ours</center></td>
		    <td><img src='./images/qualitative/ours/00004_angry.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/ours/00004_mustache.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/ours/00004_blueeyeglasses.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/ours/00004_redeyeglasses.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/ours/00004_blackeyeglasses.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/ours/00004_redbangs.png' width="150px"></img></td>
		  </tr> -->
		<!-- </tbody> -->

<!-- 		<tr>
			<th colspan="1">Input Image</td>
			<th colspan="1"><img src='./images/00018.png' width="120px"></th>
			<th></th>
			<th></th>
			<th></th>
			<th></th>
		</tr> -->
<!-- 		  <tr>
		    <td><center>Prompt</center></td>
		    <td><center>Angry ğŸ˜ </center></td>
		    <td><center>Blue Hair ğŸ”·</center></td>
		    <td><center>Black Glasses </center></td>
		    <td><center>Red Glasses</center></td>
		    <td><center>Wearing Earrings</center></td>
		    <td><center>Wearing Hat ğŸ©</center></td>
		  </tr> -->

		<!-- <tbody> -->
<!-- 		  <tr>
		    <td><center>Baseline</center></td>
		    <td><img src='./images/qualitative/baseline/00018_angry.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/baseline/00018_mustache.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/baseline/00018_blackeyeglasses.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/baseline/00018_redeyeglasses.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/baseline/00018_wearingearrings.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/baseline/00018_wearinghats.png' width="150px"></img></td>
		  </tr>
		  <tr>
		    <td><center>Ours</center></td>
		    <td><img src='./images/qualitative/ours/00018_angry.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/ours/00018_mustache.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/ours/00018_bangs.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/ours/00018_bluehair.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/ours/00018_wearingearrings.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/ours/00018_redbangs.png' width="150px"></img></td>
		  </tr> -->
		<!-- </tbody> -->
		</center>
		</table>

    <center>
    <!-- <center> -->
    <!-- <img src='./images/uw-logo-flush-web.png' width="50%"></img> -->
    </center>
    </div>

    <div class="content">
    <h2 id='quantitative-results'>ğŸ“Š Quantitative Results</h2>
        <p>We randomly sample 20 images from <a href='https://drive.google.com/drive/folders/1YxMbgiXY583eWBgt7BIbh7j1ciSkkhB_?usp=sharing'>CelebA-HQ dataset</a> and apply our methods to compare with baseline.
        	Our process involves computing embeddings for an image and text using a pre-trained <a href="https://openai.com/research/clip">CLIP model</a>.
        	We compute the mean CLIP cosine similarity between edited images and editing prompts.
    	In short, there are 20 images (20 identities) paired with 11 editing directions (220 edited images). We surpass baseline on 7/11 editing attributes: <b>63.64%</b>.</p>

    <center>
    <!-- <center> -->
    <img src='./images/quantitative.png' width="70%"></img>
    </center>
    </div>

    <div class="content">
    <h2 id='interesting-findings'>ğŸ“ Interesting Findings</h2>
    <p>During the project, we have faced major challengings and issues, in which end up with made several modifications. Here, we discuss some interesting results, both working cases and non working cases. Some results might have very strong intutive idea behind, but it turned out that it did not work in practices.</p>
    
    <!-- <center> -->
    <p><b>â“ Recovering input from noise</b></p>
    <p>Upon conducting a thorough investigation, we have discovered that the DDIM inversion method is unable to function with an additional text prompt as input. When applying the image inversion process with an extra input caption, as shown in Figure A, the resulting reconstructed image differs greatly from the input.</p>
    <center>
    	<figure>
    	<img src='./images/ddim.png' width="40%"></img>
    	<figcaption>Figure A: It is non-trivial to recover exactly one image from noise together with descriptions for that images. This is ill-posed problem as one description might fit to many images.</figcaption>
    	</figure>
    </center>
    <p><b>â“ More description is better?</b></p>
    <p>In an effort to improve the model's ability to reproduce images accurately, the decision was made to provide more textual description as input. However, the results were not as expected, and it was discovered that adding more descriptive text could actually harm the model's performance. This was attributed to the fact that a single description could be ambiguous and apply to many different images.</p>
<!--     <center>
    	<figure>
    	<img src='./images/framework-old.png' width="50%"></img>
    	<figcaption>Figure B: Do more description makes more details? The answer is suprpisingly no. More details might hurt the performance.</figcaption>
    	</figure>
    </center> -->


		<table style="width:80%">
		<!-- </thead> -->
		<tbody>
		  <tr>
		    <!-- <td><center>Baseline</center></td> -->
		    <td><center><img src='./images/desc/00000.jpeg' width="170px"></img></center></td>
		    <td><center><img src='./images/desc/nlg1.jpeg' width="170px"></img></center></td>
		    <td><center><img src='./images/desc/nlg_smile.jpeg' width="170px"></img></center></td>
		    <td><center><img src='./images/desc/nlg_glasses.jpeg' width="170px"></img></center></td>
		  </tr>
		  <tr>
		    <!-- <td><center>Prompt</center></td> -->
		    <td class="description"><center>Input Image
		    				<br>(Identity <a style="color:#FF5733;">&#8826V&#8827</a>)</center></td>
		    <td class="description"><center><a style="color:#FF5733;">&#8826V&#8827</a>
		    				<br>+
		    				<br> <a style="color:blue;">"A baby in a blue blanket" </a></center></td>
		    <td class="description"><center><a style="color:#FF5733;">&#8826V&#8827</a>
		    				<br> +
		    				<br><a style="color:blue;">"A baby in a blue blanket" </a>
		    				<br> +
		    				<br> <a style="color:green;">"smiling"</a>
		    	</center></td>
		    <td class="description"><center><a style="color:#FF5733;">&#8826V&#8827</a>
		    				<br>+
		    				<br><a style="color:blue;">"A baby in a blue blanket" </a>
		    				<br>+
		    				<br><a style="color:green;">"wearing red glasses"</a>
		    	</center></td>
		  </tr>
		</tbody>
		</table>
   <p>To illustrate this point, a Figure B was created showing how adding the description "A baby in a blue blanket" could lead the model to generate an image of a different baby than the one intended. This highlights the trade-off between accuracy and robustness, where the more the model is forced to adhere to the text prompt, the more likely it is to overlook important details in the input image.</p>
    </div>

    <div class="content">
    <h2 id='related-work'>ğŸ“– Related Works</h2>
    <p><strong>Facial Editing</strong> Over the past few years, Generative Adversarial Models (GANs), particularly <a href='https://arxiv.org/abs/1812.04948'>StyleGAN</a>, have been widely studied and used for facial image editing. However, the use of GAN models is limited to the learned latent space, which poses challenges in identifying manipulation directions within that space. In addition, GANs require an image inversion process that may accumulate errors in the editing process.
    	While GANs naturally do not support textual input, most existing works lie on exploiting <a href='https://openai.com/research/clip'>CLIP</a> to enable GANs to take text prompts as guidance.
    On the other hand, as trained on billions text-image pairs, <a href="https://ommer-lab.com/research/latent-diffusion-models/">diffusion models</a> inherently understand the text-image relationship. Diffusion models have become a better alternative to GANs due to their image quality and interpretability.</p>

    <p><strong>Diffusion Models</strong> Among diffusion models, <a href="https://ommer-lab.com/research/latent-diffusion-models/">Stable Diffusion</a> is the most popular and can generate high-quality images, but its use in editing images remains an active question.
    	The state-of-the-art model, <a href="https://www.timothybrooks.com/instruct-pix2pix/">InstructPix2Pix</a>, introduced a new synthetic dataset of before-after images to fine-tune Stable Diffusion Model.
    It has been proven that fine-tuning a model may have a negative impact on its robustness.
    InstructPix2Pix performs well under common basic settings (for example, "add glasses"), but struggles with out-of-domain editing, such as "blue hair".
    Although Original Stable Diffusion can comprehend the "blue hair" attribute, InstructPix2Pix cannot replicate this extreme editing, as it may not appear frequently enough in the training sets.</p>
    

    <p><strong>Textual Inversion</strong> <a href="https://dreambooth.github.io/">DreamBooth</a> a module that perform the textual inversion given a collection of the images as reference to a concept. The module can bind a unique identifier with a given input by fine-tuning the stable diffusion model, personalizing the general model in a few shot scenario. A new concept is introduce to the model by leveraging the prior shared knowledge from the class descriptor and then extracting the personalized knowledge by contrasting the images from other images from the same class. As a result, the identifier can well reconstruct the face given representative input.
    </p>
    <p><strong>Image Captioning</strong> Natural Language Generation (NLG) techniques will be applied to generate the linguistic contents of the input face image. Given the dataset of face-description pairs, we are able to fine-tune a pre-trained image captioning model specifically for the face class. The pre-trained <a href="https://aclanthology.org/2022.naacl-main.42/">Vision Encoder Decoder Models</a>, in which the encoder is used to encode the image, after which an autoregressive language model generates the caption.</p>

    </div>

     <div class="content">
     	<h2 id="implementation">ğŸ“¦ Miscellaneous</h2>
          <p><strong>Code</strong>: All experiments are conducted on Google Colab. We're happy to make all codes/ implementation available at:
          </p>
          	<ul>
          	  <li>Framework: <a href="https://colab.research.google.com/drive/1VufhC6-ZCc6kZj8zdRzbTR4S41XoUFd4?usp=share_link">Modified version of DreamBooth</a></li>
			  <li>Run baseline: <a href="https://colab.research.google.com/drive/1xrDwGuQiancrGTUWEl9b0lVQzwb28sJR?usp=sharing">Run InstructPix2Pix</a></li>
			  <li>Evaluation pipeline (CLIP cosine similarity): <a href="https://colab.research.google.com/drive/1C_FR0BLclkJ9C04JZcONPGiWWoP3hD5w?usp=sharing">Evaluation (CLIP)</a></li>
			</ul>
          	<!-- <img src="https://raw.githubusercontent.com/huggingface/diffusers/77aadfee6a891ab9fcfb780f87c693f7a5beeb8e/docs/source/imgs/diffusers_library.jpg" style="width:20%">
          	<img src="https://raw.githubusercontent.com/huggingface/diffusers/77aadfee6a891ab9fcfb780f87c693f7a5beeb8e/docs/source/imgs/diffusers_library.jpg" style="width:20%"> -->
<!--     </div>
    <div class="content" id="code"> -->
          <p><strong>Acknowledgements</strong>:
					<!-- We would like to thank [] for helpful discussions and feedback. Thanks to []. -->
					This website template is adopted from <a href="https://chail.github.io/patch-forensics/">this template</a> and <a href='https://pages.cs.wisc.edu/~mohitg/courses/CS766/'>this template</a>. Thank you!
		</p>
    </div>
</body>

</html>
