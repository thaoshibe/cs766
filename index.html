<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta property="og:image" content="./images/uw-crest-web.png" />
    <!-- <meta > -->
    <link rel="icon" type="image/png" href="./images/favicon.ico" />
    <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
    <meta property="og:title" content="CS766 - Final Project" />
    <meta property="og:description" content="This is the website for Final Project of CS766: Computer Vision (Spring 2023) @ UW-Madison" />
    <title>CS 766</title>
    <!-- For debug only -->
    <!-- <meta http-equiv=‚Äùrefresh‚Äù content="5" /> -->

    <!-- <link href="base.css" type="text/css" rel="StyleSheet"> -->
    <link href="index.css" type="text/css" rel="StyleSheet">
    <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
    <script type="text/javascript" src="jquery.mlens-1.0.min.js"></script>
    <script type="text/javascript" src="jquery.js"></script>
    <!-- Global site tag (gtag.js) - Google Analytics -->
		<!--
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-98008272-2"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'UA-98008272-2');
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
    </script>
		-->

</head>

<body>

    <div class="content">
			<h1>Exploring the Potential of Pretrained Stable Diffusion <br> for Manipulating Facial Attributes</h1>
            <p><center>Final Project for <a href='https://pages.cs.wisc.edu/~mohitg/courses/CS766/'> CS766: Computer Vision</a> (Spring 2023)</center></p>
        <p id="authors">
             
             <!-- <br> -->
            <font size="+2">
            <a href="https://thaoshibe.github.io/">Thao Nguyen</a>
            <a href="https://geography.wisc.edu/staff/ji-yuhan/">Yuhan Ji</a>
            <a href="https://www.linkedin.com/in/qi-yao-4a036917a/">Qi Yao</a>
<!-- 						<a href="http://people.csail.mit.edu/jwulff/">Jonas Wulff</a>
            <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> -->
            <!-- <br> -->
            </font>
            <br>
            University of Wisconsin - Madison
			<br>
            <img src='./images/uw-logo-flush-web.png' height="80px"></img>
                        <!-- <i>International Conference on Learning Representations 2021</i> -->
        </p>
				<font size="+2">
					<p style="text-align: center;">
						<a href="" target="_blank">[Video üé•]</a> &nbsp;&nbsp;&nbsp;&nbsp;
						<a href="https://docs.google.com/presentation/d/1fIEMHppZbh9MpYMAh1s8RvmF3nrTVeaJ61ZR9ZAhI0A/edit?usp=sharing">[Slides]</a> &nbsp;&nbsp;&nbsp;&nbsp;
						<a href="https://drive.google.com/file/d/1XkCAc-s3fmPjZ7NJr7rB1uIeWv57FWpY/view?usp=sharing" target="_blank">[Proposal]</a> &nbsp;&nbsp;&nbsp;&nbsp;
						<a href="https://drive.google.com/file/d/1wDn8UVKp7UW-3zqLczgaHQmNdnhiaJs7/view?usp=sharing" target="_blank">[Mid-term Report üìù]</a>
                        &nbsp;&nbsp;&nbsp;&nbsp;
						<a href="logs.html" target="_blank">[More Results üñºÔ∏è]</a>
						<!--
						<a href="TODO: youtube link?" target="_blank">[Video]</a>
						-->
					</p>
				</font>
				<font size="+1">
					<p style="text-align: center;">
						Skip to:  &nbsp;&nbsp;
						<a href="#abstract">[Abstract]</a> &nbsp;&nbsp;&nbsp;&nbsp;
						<a href="#framework">[Framework]</a> &nbsp;&nbsp;&nbsp;&nbsp;
						<a href="#evaluation">[Evaluation]</a> &nbsp;&nbsp;&nbsp;&nbsp;
                        <a href="#qualitative-results">[Qualitative Results]</a> &nbsp;&nbsp;&nbsp;&nbsp;
                        <a href="#quantitative-results">[Qualitative Results]</a> &nbsp;&nbsp;&nbsp;&nbsp;
                        <a href="#related-work">[Related Works]</a> &nbsp;&nbsp;&nbsp;&nbsp;
					</p>
				</font>
        <p>
            <!-- <img class='teaser-img' src='./images/uw-logo-flush-web.png' height="200px"></img> -->
        </p>

				<p id="abstract"><strong>Abstract: </strong>
				Facial image manipulation is the task of modifying a person's facial attributes like expressions, accessories, etc. while maintaining the identity and other irrelevant features.
                A text prompt will be provided as the guidance on how the image should be modified.
				</p>
                <center>
                <img src='./images/example_2.png' width="50%"></img>
                </center>
                <!-- <embed src='./images/example_2.pdf' width="100%" /> -->
                <!-- <iframe src='./images/example_2.pdf' style="width: 100%;height: 100%;border: none;"></iframe> -->
        <br clear="all">
    </div>
    <div class="content">
    <h2 id='framework'>üßæ Proposed Framework</h2>
    <p> Given an input image $I$ and editing text instruction $t$, our aim is to produce a new image $\hat{I}$ (shown in Figure \ref{fig:example}). While $\hat{I}$ should be edited according to the text $t$, other factors of image $I$ should be the same.
    </p>

    <p>Figure below shows the framework of the model, which contains two major parts: First, fine-tune the pre-trained stable diffusion model, that is, given one image, we need to implant the new ``word" that represents the face into the diffusion model's dictionary. Second, infer the output image conditioned on the face identifier, image description, and textual prompt.</p>
    <center>
    <!-- <center> -->
    <img src='./images/framework.png' width="80%"></img>
    </center>
    </div>

    </div>
    <div class="content">
    <h2 id='evaluation'>üìê Evaluation</h2>
    <p>Our process involves computing embeddings for an image and text using a pre-trained CLIP model \cite{clip}. Since CLIP embeddings have the property of being identical for both text and image, we can use cosine similarity on that embedding space to determine how aligned the text is with the image. For instance, if we have a photo of a woman and the text description ``smile", the cosine similarity would indicate the likelihood of the woman smiling. The cosine similarity score ranges from -1 to 1, with 1 indicating a high likelihood and -1 suggesting no relation.</p>
    <center>
    <!-- <center> -->
    <img src='./images/evaluationpipeline.png' width="50%"></img>
    </center>
    </div>
    <div class="content">
    <h2 id='qualitative-results'>üñº Qualitative Results</h2>
    <p>Qualitative results demonstrate that our method can generate images that are <b>consistent with the given text prompt</b>.
    <br>
    Especially in <b>Angry üò†, Blue Hair üî∑, and Wearing Hat üé©</b> cases.
    <br>
    Note: All test images have size 512x512, but in here we have manually downscaled them to 300x300 for quicker web rendering. For higher resolution images, please refer to this <a href='https://drive.google.com/drive/folders/1oqTnnLQ-ZzbHOvALEM7prZyCsg3lpCuN?usp=sharing'>Google Drive</a>.</p>
    </p>
		<table>
			<center>
		<!-- <thead> -->
		<tr>
			<!-- <th> </th> -->
			<th colspan="1">Input Image</td>
			<th colspan="1"><img src='./images/00007.png' width="120px"></th>
			<th></th>
			<th></th>
			<th></th>
			<th></th>
		</tr>
		  <tr>
		    <td><center>Prompt</center></td>
		    <td><center>Angry üò†</center></td>
		    <td><center>Mustache ü•∏</center></td>
		    <td><center>Blue Glasses üëì</center></td>
		    <td><center>Red Glasses üëì</center></td>
		    <td><center>Red Lipstick üíã</center></td>
		    <td><center>Blone Hair üü®</center></td>
		  </tr>
		<!-- </thead> -->
		<tbody>
		  <tr>
		    <td><center>Baseline</center></td>
		    <td><img src='./images/qualitative/baseline/00007_angry.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/baseline/00007_mustache.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/baseline/00007_blueeyeglasses.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/baseline/00007_redeyeglasses.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/baseline/00007_redlipstick.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/baseline/00007_blondehair.png' width="150px"></img></td>
		  </tr>
		  <tr>
		    <td><center>Ours</center></td>
		    <td><img src='./images/qualitative/ours/00007_angry.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/ours/00007_mustache.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/ours/00007_blueeyeglasses.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/ours/00007_redeyeglasses.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/ours/00007_redlipstick.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/ours/00007_blondehair.png' width="150px"></img></td>
		  </tr>
		</tbody>
		<tr>
			<td colspan="7"><hr></td>
		</tr>
<!-- 		<tr>
			<th colspan="1">Input Image</td>
			<th colspan="1"><img src='./images/00004.png' width="120px"></th>
			<th></th>
			<th></th>
			<th></th>
			<th></th>
		</tr> -->
<!-- 		  <tr>
		    <td><center>Prompt</center></td>
		    <td><center>Angry üò†</center></td>
		    <td><center>Blue Hair üî∑</center></td>
		    <td><center>Black Glasses </center></td>
		    <td><center>Red Glasses</center></td>
		    <td><center>Wearing Earrings</center></td>
		    <td><center>Wearing Hat üé©</center></td>
		  </tr> -->
		<!-- </thead> -->
		<!-- <tbody> -->
<!-- 		  <tr>
		    <td><center>Baseline</center></td>
		    <td><img src='./images/qualitative/baseline/00004_angry.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/baseline/00004_mustache.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/baseline/00004_blueeyeglasses.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/baseline/00007_redeyeglasses.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/baseline/00007_wearingearrings.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/baseline/00007_wearinghats.png' width="150px"></img></td>
		  </tr> -->
<!-- 		  <tr>
		    <td><center>Ours</center></td>
		    <td><img src='./images/qualitative/ours/00004_angry.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/ours/00004_mustache.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/ours/00004_blueeyeglasses.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/ours/00004_redeyeglasses.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/ours/00004_blackeyeglasses.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/ours/00004_redbangs.png' width="150px"></img></td>
		  </tr> -->
		<!-- </tbody> -->

		<tr>
			<!-- <th> </th> -->
			<th colspan="1">Input Image</td>
			<th colspan="1"><img src='./images/00018.png' width="120px"></th>
			<th></th>
			<th></th>
			<th></th>
			<th></th>
		</tr>
		  <tr>
		    <td><center>Prompt</center></td>
		    <td><center>Angry üò†</center></td>
		    <td><center>Blue Hair üî∑</center></td>
		    <td><center>Black Glasses </center></td>
		    <td><center>Red Glasses</center></td>
		    <td><center>Wearing Earrings</center></td>
		    <td><center>Wearing Hat üé©</center></td>
		  </tr>
		<!-- </thead> -->
		<tbody>
		  <tr>
		    <td><center>Baseline</center></td>
		    <td><img src='./images/qualitative/baseline/00018_angry.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/baseline/00018_mustache.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/baseline/00018_blackeyeglasses.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/baseline/00018_redeyeglasses.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/baseline/00018_wearingearrings.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/baseline/00018_wearinghats.png' width="150px"></img></td>
		  </tr>
		  <tr>
		    <td><center>Ours</center></td>
		    <td><img src='./images/qualitative/ours/00018_angry.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/ours/00018_mustache.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/ours/00018_bangs.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/ours/00018_bluehair.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/ours/00018_wearingearrings.png' width="150px"></img></td>
		    <td><img src='./images/qualitative/ours/00018_redbangs.png' width="150px"></img></td>
		  </tr>
		</tbody>
		</center>
		</table>

    <center>
    <!-- <center> -->
    <!-- <img src='./images/uw-logo-flush-web.png' width="50%"></img> -->
    </center>
    </div>

    <div class="content">
    <h2 id='quantitative-results'>üìä Quantitative Results</h2>
    <p>Our process involves computing embeddings for an image and text using a pre-trained CLIP model \cite{clip}. Since CLIP embeddings have the property of being identical for both text and image, we can use cosine similarity on that embedding space to determine how aligned the text is with the image. For instance, if we have a photo of a woman and the text description ``smile", the cosine similarity would indicate the likelihood of the woman smiling. The cosine similarity score ranges from -1 to 1, with 1 indicating a high likelihood and -1 suggesting no relation.</p>
    <center>
    <!-- <center> -->
    <img src='./images/uw-logo-flush-web.png' width="50%"></img>
    </center>
    </div>

    <div class="content">
    <h2 id='findings'>üìé Interesting Findings</h2>
    <p>During the project, we have faces major challengings and issues, in which end up with made several modifications. Here's, we discuss some interesting results, both working cases and non working cases. Some results might have very strong intutive idea behind, but it turned out that it did not works in practices.</p>
    
    <!-- <center> -->
    <p><b>‚ùì Recovering input from noise</b></p>

    <center>
    	<figure>
    	<img src='./images/ddim.png' width="40%"></img>
    	<figcaption>Figure A: It is non-trivial to recover exactly one image from noise together with descriptions for that images. This is ill-posed problem.</figcaption>
    	</figure>
    </center>
    <p><b>‚ùì More description is better?</b></p>
    <center>
    	<figure>
    	<img src='./images/framework-old.png' width="50%"></img>
    	<figcaption>Figure B: Do more description makes more details? The answer is suprpisingly no. More details might hurt the performance.</figcaption>
    	</figure>
    </center>


		<table style="width:80%">
		<!-- </thead> -->
		<tbody>
		  <tr>
		    <!-- <td><center>Baseline</center></td> -->
		    <td><center><img src='./images/desc/00000.jpeg' width="170px"></img></center></td>
		    <td><center><img src='./images/desc/nlg1.jpeg' width="170px"></img></center></td>
		    <td><center><img src='./images/desc/nlg_smile.jpeg' width="170px"></img></center></td>
		    <td><center><img src='./images/desc/nlg_glasses.jpeg' width="170px"></img></center></td>
		  </tr>
		  <tr>
		    <!-- <td><center>Prompt</center></td> -->
		    <td class="description"><center>Input Image
		    				<br>(Identity <a style="color:#FF5733;">&#8826V&#8827</a>)</center></td>
		    <td class="description"><center><a style="color:#FF5733;">&#8826V&#8827</a>
		    				<br>+
		    				<br> <a style="color:blue;">"A baby in a blue blanket" </a></center></td>
		    <td class="description"><center><a style="color:#FF5733;">&#8826V&#8827</a>
		    				<br> +
		    				<br><a style="color:blue;">"A baby in a blue blanket" </a>
		    				<br> +
		    				<br> <a style="color:green;">"smiling"</a>
		    	</center></td>
		    <td class="description"><center><a style="color:#FF5733;">&#8826V&#8827</a>
		    				<br>+
		    				<br><a style="color:blue;">"A baby in a blue blanket" </a>
		    				<br>+
		    				<br><a style="color:green;">"wearing red glasses"</a>
		    	</center></td>
		  </tr>
		</tbody>
		</table>

    </div>

    <div class="content">
    <h2 id='related-work'>üìñ Related Works</h2>
    <p><strong>Facial Editing</strong> Over the past few years, Generative Adversarial Models (GANs), particularly StyleGAN \cite{stylegan}, have been widely studied and used for facial image editing. However, the use of GAN models is limited to the learned latent space, which poses challenges in identifying manipulation directions within that space\cite{feat,styleencoder}. In addition, GANs require an image inversion process that may accumulate errors in the editing process \cite{pti,e4e}.
    While GANs naturally do not support textual input, most existing works lie on exploiting CLIP \cite{clip} to enable GANs to take text prompts as guidance \cite{styleclip}.
    On the other hand, as trained on billions text-image pairs, diffusion models inherently understand the text-image relationship. Diffusion models \cite{2021diff} have become a better alternative to GANs due to their image quality and interpretability. Text-to-image diffusion models such as DALL-E 2 \cite{ramesh2022hierarchical}, Imagen \cite{saharia2022photorealistic}, Mid-Journey\footnote{\url{https://www.midjourney.com/}}, and Stable Diffusion \cite{stablediffusion}, have proved the power of image generation under the guidance of textual prompts.</p>

    <p><strong>Diffusion Models</strong> Among diffusion models, Stable Diffusion \cite{stablediffusion} is the most popular and can generate high-quality images, but its use in editing images remains an active question.
    Diff-AE \cite{diffae} proposed to train an autoencoder module to capture the input characters. However, Diff-AE's image quality is not pleasant and manipulation is limited as it based on StyleGAN directions. The state-of-the-art model, InstructPix2Pix \cite{instructpix2pix}, introduced a new synthetic dataset of before-after images to fine-tune Stable Diffusion Model.
    It has been proven that fine-tuning a model may have a negative impact on its robustness.
    InstructPix2Pix \cite{instructpix2pix} performs well under common basic settings (for example, ``add glasses"), but struggles with out-of-domain editing, such as ``blue hair" (as depicted in Figure \ref{fig:baseline}).
    Although Original Stable Diffusion can comprehend the ``blue hair" attribute, InstructPix2Pix cannot replicate this extreme editing, as it may not appear frequently enough in the training sets.</p>
    

    <p><strong>Textual Inversion</strong> DreamBooth \cite{ruiz2022dreambooth} a module that perform the textual inversion given a collection of the images as reference to a concept. The module can bind a unique identifier with a given input by fine-tuning the stable diffusion model, personalizing the general model in a few shot scenario. A new concept is introduce to the model by leveraging the prior shared knowledge from the class descriptor and then extracting the personalized knowledge by contrasting the images from other images from the same class. As a result, the identifier can well reconstruct the face given representative input.
    </p>
    <p><strong>Image Captioning</strong> Natural Language Generation (NLG) techniques will be applied to generate the linguistic contents of the input face image. Given the dataset of face-description pairs, we are able to fine-tune a pre-trained image captioning model specifically for the face class \cite{kumar2022imagecaptioning}. The pre-trained Vision Encoder Decoder Models, in which the encoder is used to encode the image, after which an autoregressive language model generates the caption.</p>

    </div>

     <div class="content" id="code">
     	<h2>Miscellaneous</h2>
          <p><strong>Code</strong>: All experiments are conducted on Google Colab. We're happy to make all codes/ implementation available at:
          	<ul>
          	  <li>Framework: </li>
			  <li>Run baseline: <a href="https://colab.research.google.com/drive/1xrDwGuQiancrGTUWEl9b0lVQzwb28sJR?usp=sharing">Run InstructPix2Pix</a></li>
			  <li>Evaluation pipeline (CLIP cosine similarity): <a href="https://colab.research.google.com/drive/1C_FR0BLclkJ9C04JZcONPGiWWoP3hD5w?usp=sharing">Evaluation (CLIP)</a></li>
			</ul>
          	
          	<!-- <br> -->
          	<center>
          	<img src="https://raw.githubusercontent.com/huggingface/diffusers/77aadfee6a891ab9fcfb780f87c693f7a5beeb8e/docs/source/imgs/diffusers_library.jpg" style="height:7%">
          	&nbsp
          	<img src="https://upload.wikimedia.org/wikipedia/commons/9/96/Pytorch_logo.png" style="height:6%">
          	 &nbsp
          	<img src="https://static.wikia.nocookie.net/logopedia/images/d/d8/Colab.png/revision/latest?cb=20201019223838" style="height:7%">
          	<!-- <img src="https://raw.githubusercontent.com/huggingface/diffusers/77aadfee6a891ab9fcfb780f87c693f7a5beeb8e/docs/source/imgs/diffusers_library.jpg" style="width:20%">
          	<img src="https://raw.githubusercontent.com/huggingface/diffusers/77aadfee6a891ab9fcfb780f87c693f7a5beeb8e/docs/source/imgs/diffusers_library.jpg" style="width:20%"> -->
		  	
          	</center>
<!--     </div>
    <div class="content" id="code"> -->
          <p><strong>Acknowledgements</strong>:
					<!-- We would like to thank [] for helpful discussions and feedback. Thanks to []. -->
					This website template is adopted from <a href="https://chail.github.io/patch-forensics/">this template</a> and <a href='https://pages.cs.wisc.edu/~mohitg/courses/CS766/'>this template</a>. Thank you!</p>
    </div>
</body>

</html>
